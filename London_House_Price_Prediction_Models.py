# -*- coding: utf-8 -*-
"""Copy of Model3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qmruagZdWnYpAYOXfXJF3fL4YItWm3Oy

# **Importing libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
import xgboost as xgb
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer

"""# **Dataframe**"""

df_tier4 = pd.read_csv('drive/MyDrive/Dissertation/df_tier4_Visuals_10yrs.csv')

df_tier4.dtypes

district_counts = df_tier4['district'].value_counts()
print(district_counts)

# Drop the "Unnamed: 0" and "unique_id" columns
df_tier4.drop(["Unnamed: 0", "unique_id"], axis=1, inplace=True)

df_tier4

"""# **Label Encoding**"""

from sklearn.preprocessing import LabelEncoder
import pandas as pd

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Initialize a dictionary to store label encoded values
encoded_values = {}

# Iterate over each column in the DataFrame
for feature in df_tier4.columns:
    if df_tier4[feature].dtype == 'object':
        # Encode the categorical variable
        df_tier4[feature] = label_encoder.fit_transform(df_tier4[feature])

        # Store the encoded values in the dictionary
        encoded_values[feature] = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))

# Display the DataFrame after label encoding
print(df_tier4.head())

# Display the dictionary containing encoded values
print(encoded_values)

encoded_values

df_tier4

"""# **selecting target and features variable**"""

# Define target variable
target = 'price_paid'

# Define feature variables (excluding the target variable)
features = df_tier4.columns[df_tier4.columns != target].tolist()

"""# **Splitting the dataset**"""

# Ensure data is sorted by date
df_tier4 = df_tier4.sort_values(by=['year', 'month', 'day'])

# Splitting data into training and testing sets
train_size = int(0.7 * len(df_tier4))
train_data = df_tier4[:train_size]
test_data = df_tier4[train_size:]

# Define features and target variable
X_train = train_data.drop(columns=['price_paid'])
y_train = train_data['price_paid']
X_test = test_data.drop(columns=['price_paid'])
y_test = test_data['price_paid']

"""# **Linear Regression**"""

# Initialize and train the Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Model evaluation
y_pred = lr_model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)
r2 = r2_score(y_test, y_pred)

print("R-squared:", r2)
print("Root Mean Squared Error:", rmse)
print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)

"""## **Cross validation**"""

# Define custom scoring functions for R-squared, RMSE, MSE, and MAE
def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

def mse(y_true, y_pred):
    return mean_squared_error(y_true, y_pred)

def mae(y_true, y_pred):
    return mean_absolute_error(y_true, y_pred)

# Make scorer objects for each custom scoring function
r2_scorer = make_scorer(r2_score)
rmse_scorer = make_scorer(rmse, greater_is_better=False)  # RMSE is better when lower
mse_scorer = make_scorer(mse, greater_is_better=False)    # MSE is better when lower
mae_scorer = make_scorer(mae, greater_is_better=False)    # MAE is better when lower

# Perform cross-validation with custom scoring functions
cv_scores = cross_val_score(lr_model, X_train, y_train, cv=5, scoring='r2')
cv_r2 = cross_val_score(lr_model, X_train, y_train, cv=5, scoring=r2_scorer)
cv_rmse = -cross_val_score(lr_model, X_train, y_train, cv=5, scoring=rmse_scorer)
cv_mse = -cross_val_score(lr_model, X_train, y_train, cv=5, scoring=mse_scorer)
cv_mae = -cross_val_score(lr_model, X_train, y_train, cv=5, scoring=mae_scorer)

# Print the cross-validation scores
print("Cross-Validation Scores:", cv_scores)
print("Cross-Validation R-squared:", cv_r2)
print("Mean R-squared:", np.mean(cv_r2))
print("Cross-Validation RMSE:", cv_rmse)
print("Mean RMSE:", np.mean(cv_rmse))
print("Cross-Validation MSE:", cv_mse)
print("Mean MSE:", np.mean(cv_mse))
print("Cross-Validation MAE:", cv_mae)
print("Mean MAE:", np.mean(cv_mae))

# Randomly select 250 samples from the test set
np.random.seed(42)
sample_indices = np.random.choice(len(y_test), size=250, replace=False)
sample_y_test = y_test.iloc[sample_indices]
sample_y_pred = y_pred[sample_indices]

# Plot the sample of predicted prices versus actual prices
plt.figure(figsize=(18, 8))
plt.plot(range(len(sample_y_test)), sample_y_test, label='Actual Prices', color='red')
plt.plot(range(len(sample_y_test)), sample_y_pred, label='LR Default Predictions', color='green')
plt.xlabel('Sample Index')
plt.ylabel('Price Paid (Â£)')
plt.title('250 Sample Predicted Prices vs Actual Prices (Linear Regression)')
plt.legend()
plt.show()

"""# **XGBoost**

# **Default**
"""

# Train XGBoost model
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, max_depth=5, learning_rate=0.1)

xgb_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_default = xgb_model.predict(X_test)

# Evaluate the model's performance
mse = mean_squared_error(y_test, y_pred_default)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred_default)
mae = mean_absolute_error(y_test, y_pred_default)

print("Test R-squared:", r2)
print("Test RMSE:", rmse)
print("Test MSE:", mse)
print("Test MAE:", mae)

"""## **Features importance**"""

# Obtain feature importances
feature_importances = xgb_model.feature_importances_

# Create a DataFrame to store feature importances along with their names
feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance values in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Visualize feature importances
plt.figure(figsize=(10, 10))
sns.barplot(data=feature_importance_df, x='Importance', y='Feature')
plt.title('Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

"""## **Cross Validation**"""

# Define custom scoring functions for R-squared, RMSE, MSE, and MAE
def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

def mse(y_true, y_pred):
    return mean_squared_error(y_true, y_pred)

def mae(y_true, y_pred):
    return mean_absolute_error(y_true, y_pred)

# Make scorer objects for each custom scoring function
r2_scorer = make_scorer(r2_score)
rmse_scorer = make_scorer(rmse, greater_is_better=False)  # RMSE is better when lower
mse_scorer = make_scorer(mse, greater_is_better=False)    # MSE is better when lower
mae_scorer = make_scorer(mae, greater_is_better=False)    # MAE is better when lower

# Perform cross-validation with custom scoring functions
cv_r2 = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring=r2_scorer)
cv_rmse = -cross_val_score(xgb_model, X_train, y_train, cv=5, scoring=rmse_scorer)
cv_mse = -cross_val_score(xgb_model, X_train, y_train, cv=5, scoring=mse_scorer)
cv_mae = -cross_val_score(xgb_model, X_train, y_train, cv=5, scoring=mae_scorer)

# Print the cross-validation scores
print("Cross-Validation R-squared:", cv_r2)
print("Mean R-squared:", np.mean(cv_r2))
print("Cross-Validation RMSE:", cv_rmse)
print("Mean RMSE:", np.mean(cv_rmse))
print("Cross-Validation MSE:", cv_mse)
print("Mean MSE:", np.mean(cv_mse))
print("Cross-Validation MAE:", cv_mae)
print("Mean MAE:", np.mean(cv_mae))

"""# **Optimization**"""

# Define the hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.1, 0.01, 0.001]
}

# Instantiate XGBoost model
xgb_model = xgb.XGBRegressor(objective='reg:squarederror')

# Perform grid search cross-validation
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Evaluate the best model on the holdout test set
best_xgb_model = grid_search.best_estimator_
y_pred_optimized = best_xgb_model.predict(X_test)

r2 = r2_score(y_test, y_pred_optimized)
mse = mean_squared_error(y_test, y_pred_optimized)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred_optimized)

print("Test R-squared:", r2)
print("Test RMSE:", rmse)
print("Test MSE:", mse)
print("Test MAE:", mae)

import xgboost as xgb
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Initialize a new XGBoost model with the best hyperparameters
best_xgb_model = xgb.XGBRegressor(
    learning_rate=0.1,
    max_depth=7,
    n_estimators=300
)

# Train the model on your training data
best_xgb_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_optimized = best_xgb_model.predict(X_test)

# Evaluate the model performance
r2 = r2_score(y_test, y_pred_optimized)
mse = mean_squared_error(y_test, y_pred_optimized)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred_optimized)

# Print the evaluation metrics
print("Test R-squared:", r2)
print("Test RMSE:", rmse)
print("Test MSE:", mse)
print("Test MAE:", mae)

"""## **Features importance**"""

# Obtain feature importances
feature_importances = best_xgb_model.feature_importances_

# Create a DataFrame to store feature importances along with their names
feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance values in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Visualize feature importances
plt.figure(figsize=(10, 10))
sns.barplot(data=feature_importance_df, x='Importance', y='Feature')
plt.title('Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

"""## **Cross validation**"""

# Define custom scoring functions for R-squared, RMSE, MSE, and MAE
def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

def mse(y_true, y_pred):
    return mean_squared_error(y_true, y_pred)

def mae(y_true, y_pred):
    return mean_absolute_error(y_true, y_pred)

# Make scorer objects for each custom scoring function
r2_scorer = make_scorer(r2_score)
rmse_scorer = make_scorer(rmse, greater_is_better=False)  # RMSE is better when lower
mse_scorer = make_scorer(mse, greater_is_better=False)    # MSE is better when lower
mae_scorer = make_scorer(mae, greater_is_better=False)    # MAE is better when lower

# Perform cross-validation with custom scoring functions
cv_scores = cross_val_score(best_xgb_model, X_train, y_train, cv=5, scoring='r2')
cv_r2 = cross_val_score(best_xgb_model, X_train, y_train, cv=5, scoring=r2_scorer)
cv_rmse = -cross_val_score(best_xgb_model, X_train, y_train, cv=5, scoring=rmse_scorer)
cv_mse = -cross_val_score(best_xgb_model, X_train, y_train, cv=5, scoring=mse_scorer)
cv_mae = -cross_val_score(best_xgb_model, X_train, y_train, cv=5, scoring=mae_scorer)

# Print the cross-validation scores
print("Cross-Validation Scores:", cv_scores)
print("Cross-Validation R-squared:", cv_r2)
print("Mean R-squared:", np.mean(cv_r2))
print("Cross-Validation RMSE:", cv_rmse)
print("Mean RMSE:", np.mean(cv_rmse))
print("Cross-Validation MSE:", cv_mse)
print("Mean MSE:", np.mean(cv_mse))
print("Cross-Validation MAE:", cv_mae)
print("Mean MAE:", np.mean(cv_mae))

"""# **Visualization**"""

# Plot actual prices versus default prediction versus optimized prediction
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_default, color='blue', label='Default Prediction')
plt.scatter(y_test, y_pred_optimized, color='green', label='Optimized Prediction')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, color='red')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual Prices vs Default Prediction vs Optimized Prediction (XGBoost)')
plt.legend()
plt.show()

# Randomly select 250 samples from the test set
np.random.seed(42)
sample_indices = np.random.choice(len(y_test), size=250, replace=False)
sample_y_test = y_test.iloc[sample_indices]
sample_y_pred_default = y_pred_default[sample_indices]
sample_y_pred_optimized = y_pred_optimized[sample_indices]

# Plot the sample of predicted prices versus actual prices
plt.figure(figsize=(18, 8))
plt.plot(range(len(sample_y_test)), sample_y_test, label='Actual Prices', color='blue')
plt.plot(range(len(sample_y_test)), sample_y_pred_default, label='DT Default Predictions', color='green')
plt.plot(range(len(sample_y_test)), sample_y_pred_optimized, label='DT Optimized Predictions', color='red')
plt.xlabel('Sample Index')
plt.ylabel('Price Paid (Â£)')
plt.title('250 Sample Predicted Prices vs Actual Prices (XGBoost)')
plt.legend()
plt.show()

"""# **XAI - XGBoost**"""

pip install shap

import shap

# Initialize the SHAP explainer with the best model and training data
explainer = shap.Explainer(best_xgb_model, X_train)
# Calculate SHAP values for the test data
shap_values = explainer.shap_values(X_test)

explainer.expected_value, np.mean(y_test)

"""## **Global interpretability**"""

# Visualize SHAP summary plot
shap.summary_plot(shap_values, X_test)

# Create summary plot with custom colors and increased size
plt.figure(figsize=(12, 8))  # Set the size of the plot
shap.summary_plot(shap_values, X_test, plot_type='bar', color='coral')
plt.show()

shap.initjs()
# Check feature impact on individual predictions
shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0])

"""## **Shap analysis - Local**"""

# Create a SHAP explainer for your model
explainer = shap.TreeExplainer(best_xgb_model)

# Choose the index of the instance you want to explain
index_to_explain = 0

# Create the Explanation object
expl = shap.Explanation(values=shap_values[index_to_explain], base_values=explainer.expected_value, data=X_test.iloc[index_to_explain])

# Create the waterfall plot for the selected instance
shap.plots.waterfall(expl, max_display=99, show=False)

# Save the plot to a file
plt.gcf()
plt.savefig("waterfall_plot.png")
plt.show()

# Create a SHAP explainer for your model
explainer = shap.TreeExplainer(best_xgb_model)

# Choose the index of the instance you want to explain
index_to_explain = 1

# Create the Explanation object
expl = shap.Explanation(values=shap_values[index_to_explain], base_values=explainer.expected_value, data=X_test.iloc[index_to_explain])

# Create the waterfall plot for the selected instance
shap.plots.waterfall(expl, max_display=99, show=False)

# Save the plot to a file
plt.gcf()
plt.savefig("waterfall_plot.png")
plt.show()

# Choose a specific data point for explanation (e.g., the first data point)
data_point_index = 0
data_point_shap_values = shap_values[data_point_index]

# Calculate the 95th percentile of the SHAP values
percentile_95 = np.percentile(np.abs(data_point_shap_values), 95)

# Filter features based on percentile range
selected_features = X_test.columns[np.abs(data_point_shap_values) >= percentile_95]
selected_shap_values = data_point_shap_values[np.abs(data_point_shap_values) >= percentile_95]

# Generate explanation for the chosen data point with selected features
shap.initjs()  # Initialize Javascript for visualization (required for some plots)
shap.force_plot(explainer.expected_value, selected_shap_values, X_test.iloc[data_point_index][selected_features], matplotlib=True)

# Choose a specific data point for explanation (e.g., the first data point)
data_point_index = 0
data_point_shap_values = shap_values[data_point_index]

# Calculate the 90th percentile of the SHAP values
percentile_90 = np.percentile(np.abs(data_point_shap_values), 90)

# Filter features based on percentile range
selected_features = X_test.columns[np.abs(data_point_shap_values) >= percentile_90]
selected_shap_values = data_point_shap_values[np.abs(data_point_shap_values) >= percentile_90]

# Generate explanation for the chosen data point with selected features
shap.initjs()  # Initialize Javascript for visualization (required for some plots)
shap.force_plot(explainer.expected_value, selected_shap_values, X_test.iloc[data_point_index][selected_features], matplotlib=True)

# Choose a specific data point for explanation (e.g., the first data point)
data_point_index = 0
data_point_shap_values = shap_values[data_point_index]

# Calculate the 75th percentile of the SHAP values
percentile_75 = np.percentile(np.abs(data_point_shap_values), 75)

# Filter features based on percentile range
selected_features = X_test.columns[np.abs(data_point_shap_values) >= percentile_75]
selected_shap_values = data_point_shap_values[np.abs(data_point_shap_values) >= percentile_75]

# Generate explanation for the chosen data point with selected features
shap.initjs()  # Initialize Javascript for visualization (required for some plots)
shap.force_plot(explainer.expected_value, selected_shap_values, X_test.iloc[data_point_index][selected_features], matplotlib=True)

# Choose a specific data point for explanation (e.g., the first data point)
data_point_index = 0
data_point_shap_values = shap_values[data_point_index]

# Calculate the 50th percentile of the SHAP values
percentile_50 = np.percentile(np.abs(data_point_shap_values), 50)

# Filter features based on percentile range
selected_features = X_test.columns[np.abs(data_point_shap_values) >= percentile_50]
selected_shap_values = data_point_shap_values[np.abs(data_point_shap_values) >= percentile_50]

# Generate explanation for the chosen data point with selected features
shap.initjs()  # Initialize Javascript for visualization (required for some plots)
shap.force_plot(explainer.expected_value, selected_shap_values, X_test.iloc[data_point_index][selected_features], matplotlib=True)

# Choose a specific data point for explanation
data_point_index = 0
data_point_shap_values = shap_values[data_point_index]

# Calculate the 25th percentile of the SHAP values
percentile_25 = np.percentile(np.abs(data_point_shap_values), 25)

# Filter features based on percentile range
selected_features = X_test.columns[np.abs(data_point_shap_values) >= percentile_25]
selected_shap_values = data_point_shap_values[np.abs(data_point_shap_values) >= percentile_25]

# Generate explanation for the chosen data point with selected features
shap.initjs()  # Initialize Javascript for visualization (required for some plots)
shap.force_plot(explainer.expected_value, selected_shap_values, X_test.iloc[data_point_index][selected_features], matplotlib=True)

# Choose a specific data point for explanation
data_point_index = 1
data_point_shap_values = shap_values[data_point_index]

# Calculate the 95th percentile of the SHAP values
percentile_95 = np.percentile(np.abs(data_point_shap_values), 95)

# Filter features based on percentile range
selected_features = X_test.columns[np.abs(data_point_shap_values) >= percentile_95]
selected_shap_values = data_point_shap_values[np.abs(data_point_shap_values) >= percentile_95]

# Generate explanation for the chosen data point with selected features
shap.initjs()  # Initialize Javascript for visualization (required for some plots)
shap.force_plot(explainer.expected_value, selected_shap_values, X_test.iloc[data_point_index][selected_features], matplotlib=True)

# Choose a specific data point for explanation
data_point_index = 1
data_point_shap_values = shap_values[data_point_index]

# Calculate the 90th percentile of the SHAP values
percentile_90 = np.percentile(np.abs(data_point_shap_values), 90)

# Filter features based on percentile range
selected_features = X_test.columns[np.abs(data_point_shap_values) >= percentile_90]
selected_shap_values = data_point_shap_values[np.abs(data_point_shap_values) >= percentile_90]

# Generate explanation for the chosen data point with selected features
shap.initjs()  # Initialize Javascript for visualization (required for some plots)
shap.force_plot(explainer.expected_value, selected_shap_values, X_test.iloc[data_point_index][selected_features], matplotlib=True)

# Choose a specific data point for explanation
data_point_index = 1
data_point_shap_values = shap_values[data_point_index]

# Calculate the 75th percentile of the SHAP values
percentile_75 = np.percentile(np.abs(data_point_shap_values), 75)

# Filter features based on percentile range
selected_features = X_test.columns[np.abs(data_point_shap_values) >= percentile_75]
selected_shap_values = data_point_shap_values[np.abs(data_point_shap_values) >= percentile_75]

# Generate explanation for the chosen data point with selected features
shap.initjs()  # Initialize Javascript for visualization (required for some plots)
shap.force_plot(explainer.expected_value, selected_shap_values, X_test.iloc[data_point_index][selected_features], matplotlib=True)

# Choose a specific data point for explanation
data_point_index = 1
data_point_shap_values = shap_values[data_point_index]

# Calculate the 50th percentile of the SHAP values
percentile_50 = np.percentile(np.abs(data_point_shap_values), 50)

# Filter features based on percentile range
selected_features = X_test.columns[np.abs(data_point_shap_values) >= percentile_50]
selected_shap_values = data_point_shap_values[np.abs(data_point_shap_values) >= percentile_50]

# Generate explanation for the chosen data point with selected features
shap.initjs()  # Initialize Javascript for visualization (required for some plots)
shap.force_plot(explainer.expected_value, selected_shap_values, X_test.iloc[data_point_index][selected_features], matplotlib=True)

# Choose a specific data point for explanation
data_point_index = 1
data_point_shap_values = shap_values[data_point_index]

# Calculate the 25th percentile of the SHAP values
percentile_25 = np.percentile(np.abs(data_point_shap_values), 25)

# Filter features based on percentile range
selected_features = X_test.columns[np.abs(data_point_shap_values) >= percentile_25]
selected_shap_values = data_point_shap_values[np.abs(data_point_shap_values) >= percentile_25]

# Generate explanation for the chosen data point with selected features
shap.initjs()  # Initialize Javascript for visualization (required for some plots)
shap.force_plot(explainer.expected_value, selected_shap_values, X_test.iloc[data_point_index][selected_features], matplotlib=True)

"""## **Feature Importance**"""

# Aggregate SHAP values
mean_abs_shap = np.mean(np.abs(shap_values), axis=0)
feature_names = X_train.columns

# Create a DataFrame to store feature importance
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Mean_abs_SHAP': mean_abs_shap})
feature_importance_df = feature_importance_df.sort_values(by='Mean_abs_SHAP', ascending=True)

# Visualize feature importance
plt.figure(figsize=(16, 10))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Mean_abs_SHAP'], color='turquoise')
plt.xlabel('Mean Absolute SHAP Value')
plt.ylabel('Feature')
plt.title('Feature Importance')
plt.show()

"""## **Individual Prediction Explanations**"""

# Choose a specific data point for explanation (e.g., the first data point)
data_point_index = 0
data_point_shap_values = shap_values[data_point_index]

# Generate explanation for the chosen data point
shap.initjs()  # Initialize Javascript for visualization (required for some plots)
shap.force_plot(explainer.expected_value, data_point_shap_values, X_test.iloc[data_point_index], matplotlib=True)

# Choose a specific data point for explanation (e.g., the first data point)
data_point_index = 1
data_point_shap_values = shap_values[data_point_index]

# Generate explanation for the chosen data point
shap.initjs()  # Initialize Javascript for visualization (required for some plots)
shap.force_plot(explainer.expected_value, data_point_shap_values, X_test.iloc[data_point_index], matplotlib=True)

# Choose a specific data point for explanation
data_point_index = 2
data_point_shap_values = shap_values[data_point_index]

# Generate explanation for the chosen data point
shap.initjs()  # Initialize Javascript for visualization (required for some plots)
shap.force_plot(explainer.expected_value, data_point_shap_values, X_test.iloc[data_point_index], matplotlib=True)

# Choose a specific data point for explanation
data_point_index = 3
data_point_shap_values = shap_values[data_point_index]

# Generate explanation for the chosen data point
shap.initjs()  # Initialize Javascript for visualization (required for some plots)
shap.force_plot(explainer.expected_value, data_point_shap_values, X_test.iloc[data_point_index], matplotlib=True)

# Choose a specific data point for explanation
data_point_index = 4
data_point_shap_values = shap_values[data_point_index]

# Generate explanation for the chosen data point
shap.initjs()  # Initialize Javascript for visualization (required for some plots)
shap.force_plot(explainer.expected_value, data_point_shap_values, X_test.iloc[data_point_index], matplotlib=True)

"""## **Dependence Plots**"""

# Choose a feature for the dependence plot (e.g., 'Latitude')
feature_index = 'Latitude'

# Generate dependence plot
shap.dependence_plot(feature_index, shap_values, X_test)

# Choose a feature for the dependence plot (e.g., 'Longitude')
feature_index = 'Longitude'

# Generate dependence plot
shap.dependence_plot(feature_index, shap_values, X_test)

shap.dependence_plot('GDP', shap_values, X_test)

# Define a list of features to analyze
features_to_plot = ['day', 'month', 'year', 'quarter', 'Latitude', 'Longitude', 'deed_date', 'property_type', 'new_build', 'estate_type', 'transaction_category',
            'nearest_bus_stop_distance', 'count_bus_stops_within_500m', 'count_bus_stops_within_1km', 'count_bus_stops_within_2km', 'town', 'district', 'county',
            'nearest_rail_distance', 'count_rail_station_within_500m', 'count_rail_station_within_1km', 'count_rail_station_within_2km', 'postcode',
            'nearest_supermart_distance', 'count_supermart_within_500m', 'count_supermart_within_1km', 'count_supermart_within_2km',
            'GDP', 'employment rate', 'unemployment rate', 'inflation rate', 'interest rate']

# Create dependence plots for each feature
for feature_index in features_to_plot:
    shap.dependence_plot(feature_index, shap_values, X_test)

"""# **One hot encoding**"""

df_tier5 = pd.read_csv('drive/MyDrive/Dissertation/df_tier4_Visuals_10yrs.csv')

df_tier5.dtypes

# Drop the "Unnamed: 0" and "unique_id" columns
df_tier5.drop(["Unnamed: 0", "unique_id"], axis=1, inplace=True)

# Perform one-hot encoding for categorical variables
df_tier5 = pd.get_dummies(df_tier5, columns=['deed_date', 'property_type', 'new_build', 'estate_type', 'transaction_category', 'town',
                                             'district', 'county', 'postcode'])

df_tier5['district'].unique()

"""# **RandomForest Regressor**

# **Default**
"""

# Initialize and train the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Model evaluation
y_pred_default = rf_model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred_default)
mse = mean_squared_error(y_test, y_pred_default)
rmse = mean_squared_error(y_test, y_pred_default, squared=False)
r2 = r2_score(y_test, y_pred_default)

print("R-squared:", r2)
print("Root Mean Squared Error:", rmse)
print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)

"""## **Features Importance**"""

# Obtain feature importances
feature_importances = rf_model.feature_importances_

# Create a DataFrame to store feature importances along with their names
feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance values in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Visualize feature importances
plt.figure(figsize=(10, 10))
sns.barplot(data=feature_importance_df, x='Importance', y='Feature')
plt.title('Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

"""## **Cross validation**"""

# Define custom scoring functions for R-squared, RMSE, MSE, and MAE
def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

def mse(y_true, y_pred):
    return mean_squared_error(y_true, y_pred)

def mae(y_true, y_pred):
    return mean_absolute_error(y_true, y_pred)

# Make scorer objects for each custom scoring function
r2_scorer = make_scorer(r2_score)
rmse_scorer = make_scorer(rmse, greater_is_better=False)  # RMSE is better when lower
mse_scorer = make_scorer(mse, greater_is_better=False)    # MSE is better when lower
mae_scorer = make_scorer(mae, greater_is_better=False)    # MAE is better when lower

# Perform cross-validation with custom scoring functions
cv_r2 = cross_val_score(rf_model, X_train, y_train, cv=5, scoring=r2_scorer)
cv_rmse = -cross_val_score(rf_model, X_train, y_train, cv=5, scoring=rmse_scorer)
cv_mse = -cross_val_score(rf_model, X_train, y_train, cv=5, scoring=mse_scorer)
cv_mae = -cross_val_score(rf_model, X_train, y_train, cv=5, scoring=mae_scorer)

# Print the cross-validation scores
print("Cross-Validation R-squared:", cv_r2)
print("Mean R-squared:", np.mean(cv_r2))
print("Cross-Validation RMSE:", cv_rmse)
print("Mean RMSE:", np.mean(cv_rmse))
print("Cross-Validation MSE:", cv_mse)
print("Mean MSE:", np.mean(cv_mse))
print("Cross-Validation MAE:", cv_mae)
print("Mean MAE:", np.mean(cv_mae))

"""# **Optimization**"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

# Define the Random Forest Regressor model
rf = RandomForestRegressor()

# Define the hyperparameter grid
param_dist = {
    'n_estimators': randint(50, 200),  # Number of trees in the forest
    'max_features': ['auto', 'sqrt'],  # Number of features to consider at every split
    'max_depth': randint(10, 100),  # Maximum depth of the tree
    'min_samples_split': randint(2, 10),  # Minimum number of samples required to split a node
    'min_samples_leaf': randint(1, 10),  # Minimum number of samples required at each leaf node
    'bootstrap': [True, False]  # Method of selecting samples for training each tree
}

# Define the RandomizedSearchCV object
random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)

# Perform the Randomized Search
random_search.fit(X_train, y_train)

# Get the best parameters
best_params = random_search.best_params_
print("Best Hyperparameters:", best_params)

# Get the best model
best_model = random_search.best_estimator_

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor
from scipy.stats import randint

# Define hyperparameter grid with reduced parameters
param_dist = {
    'n_estimators': randint(50, 200),  # Reduced range for number of trees
    'max_depth': randint(5, 20),        # Reduced range for maximum depth
    'min_samples_split': randint(2, 10),# Reduced range for minimum samples split
    'min_samples_leaf': randint(1, 5)   # Reduced range for minimum samples leaf
}

# Instantiate RandomForestRegressor
rf = RandomForestRegressor()

# Instantiate RandomizedSearchCV with reduced parameters
random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,
                                   n_iter=10, cv=5, scoring='neg_mean_squared_error',
                                   random_state=42, n_jobs=-1)

# Fit RandomizedSearchCV to your data
random_search.fit(X_train, y_train)

# Get best parameters and best estimator
best_params = random_search.best_params_
best_estimator = random_search.best_estimator_

# Best parameters
print("Best Parameters:", best_params)
print("Best Estimeters:", best_estimator)

# Use the best estimator for further analysis or prediction

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import xgboost as xgb

# Initialize a new RandomForestRegressor with the best hyperparameters
best_rf_model = RandomForestRegressor(
    max_depth=17,
    min_samples_leaf=4,
    min_samples_split=2,
    n_estimators=98
)

# Train the model on your training data
best_rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_optimized = best_rf_model.predict(X_test)

# Evaluate the model performance
r2 = r2_score(y_test, y_pred_optimized)
mse = mean_squared_error(y_test, y_pred_optimized)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred_optimized)

# Print the evaluation metrics
print("Test R-squared:", r2)
print("Test RMSE:", rmse)
print("Test MSE:", mse)
print("Test MAE:", mae)

# Model evaluation
y_pred_optimized = best_rf_model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred_optimized)
mse = mean_squared_error(y_test, y_pred_optimized)
rmse = mean_squared_error(y_test, y_pred_optimized, squared=False)
r2 = r2_score(y_test, y_pred_optimized)

print("R-squared:", r2)
print("Root Mean Squared Error:", rmse)
print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)

"""## **Features importance**"""

# Obtain feature importances
feature_importances = best_rf_model.feature_importances_

# Create a DataFrame to store feature importances along with their names
feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance values in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Visualize feature importances
plt.figure(figsize=(10, 10))
sns.barplot(data=feature_importance_df, x='Importance', y='Feature')
plt.title('Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

"""## **Cross validation**"""

# Define custom scoring functions for R-squared, RMSE, MSE, and MAE
def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

def mse(y_true, y_pred):
    return mean_squared_error(y_true, y_pred)

def mae(y_true, y_pred):
    return mean_absolute_error(y_true, y_pred)

# Make scorer objects for each custom scoring function
r2_scorer = make_scorer(r2_score)
rmse_scorer = make_scorer(rmse, greater_is_better=False)  # RMSE is better when lower
mse_scorer = make_scorer(mse, greater_is_better=False)    # MSE is better when lower
mae_scorer = make_scorer(mae, greater_is_better=False)    # MAE is better when lower

# Perform cross-validation with custom scoring functions
cv_r2 = cross_val_score(best_rf_model, X_train, y_train, cv=5, scoring=r2_scorer)
cv_rmse = -cross_val_score(best_rf_model, X_train, y_train, cv=5, scoring=rmse_scorer)
cv_mse = -cross_val_score(best_rf_model, X_train, y_train, cv=5, scoring=mse_scorer)
cv_mae = -cross_val_score(best_rf_model, X_train, y_train, cv=5, scoring=mae_scorer)

# Print the cross-validation scores
print("Cross-Validation R-squared:", cv_r2)
print("Mean R-squared:", np.mean(cv_r2))
print("Cross-Validation RMSE:", cv_rmse)
print("Mean RMSE:", np.mean(cv_rmse))
print("Cross-Validation MSE:", cv_mse)
print("Mean MSE:", np.mean(cv_mse))
print("Cross-Validation MAE:", cv_mae)
print("Mean MAE:", np.mean(cv_mae))

"""# **Visualization**"""

# Plot actual prices versus default prediction versus optimized prediction
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_default, color='blue', label='Default Prediction')
plt.scatter(y_test, y_pred_optimized, color='green', label='Optimized Prediction')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, color='red')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual Prices vs Default Prediction vs Optimized Prediction (RandomForest Regressor)')
plt.legend()
plt.show()

# Randomly select 250 samples from the test set
np.random.seed(42)
sample_indices = np.random.choice(len(y_test), size=250, replace=False)
sample_y_test = y_test.iloc[sample_indices]
sample_y_pred_default = y_pred_default[sample_indices]
sample_y_pred_optimized = y_pred_optimized[sample_indices]

# Plot the sample of predicted prices versus actual prices
plt.figure(figsize=(18, 8))
plt.plot(range(len(sample_y_test)), sample_y_test, label='Actual Prices', color='blue')
plt.plot(range(len(sample_y_test)), sample_y_pred_default, label='DT Default Predictions', color='green')
plt.plot(range(len(sample_y_test)), sample_y_pred_optimized, label='DT Optimized Predictions', color='red')
plt.xlabel('Sample Index')
plt.ylabel('Price Paid (Â£)')
plt.title('250 Sample Predicted Prices vs Actual Prices (RandomForest Regressor)')
plt.legend()
plt.show()

"""# **Decision Tree Regressor**

# **Default**
"""

# Initialize and train the Decision Tree Regressor model
dt_model = DecisionTreeRegressor(random_state=42)
dt_model.fit(X_train, y_train)

# Model evaluation
y_pred_default = dt_model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred_default)
mse = mean_squared_error(y_test, y_pred_default)
rmse = mean_squared_error(y_test, y_pred_default, squared=False)
r2 = r2_score(y_test, y_pred_default)

print("R-squared:", r2)
print("Root Mean Squared Error:", rmse)
print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)

"""## **Feature importance**"""

# Obtain feature importances
feature_importances = dt_model.feature_importances_

# Create a DataFrame to store feature importances along with their names
feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance values in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Visualize feature importances
plt.figure(figsize=(10, 10))
sns.barplot(data=feature_importance_df, x='Importance', y='Feature')
plt.title('Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

"""## **Cross validation**"""

# Define custom scoring functions for R-squared, RMSE, MSE, and MAE
def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

def mse(y_true, y_pred):
    return mean_squared_error(y_true, y_pred)

def mae(y_true, y_pred):
    return mean_absolute_error(y_true, y_pred)

# Make scorer objects for each custom scoring function
r2_scorer = make_scorer(r2_score)
rmse_scorer = make_scorer(rmse, greater_is_better=False)  # RMSE is better when lower
mse_scorer = make_scorer(mse, greater_is_better=False)    # MSE is better when lower
mae_scorer = make_scorer(mae, greater_is_better=False)    # MAE is better when lower

# Perform cross-validation with custom scoring functions
cv_r2 = cross_val_score(dt_model, X_train, y_train, cv=5, scoring=r2_scorer)
cv_rmse = -cross_val_score(dt_model, X_train, y_train, cv=5, scoring=rmse_scorer)
cv_mse = -cross_val_score(dt_model, X_train, y_train, cv=5, scoring=mse_scorer)
cv_mae = -cross_val_score(dt_model, X_train, y_train, cv=5, scoring=mae_scorer)

# Print the cross-validation scores
print("Cross-Validation R-squared:", cv_r2)
print("Mean R-squared:", np.mean(cv_r2))
print("Cross-Validation RMSE:", cv_rmse)
print("Mean RMSE:", np.mean(cv_rmse))
print("Cross-Validation MSE:", cv_mse)
print("Mean MSE:", np.mean(cv_mse))
print("Cross-Validation MAE:", cv_mae)
print("Mean MAE:", np.mean(cv_mae))

"""# **Optimization**"""

# Hyperparameter Tuning
param_grid = {
    "max_depth": [None, 5, 10, 15],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 4],
    "max_features": ["auto", "sqrt", "log2"]
}

grid_search = GridSearchCV(estimator=dt_model, param_grid=param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

# Best estimator (model)
best_dt_model = grid_search.best_estimator_

# Model evaluation
y_pred_optimized = best_dt_model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred_optimized)
mse = mean_squared_error(y_test, y_pred_optimized)
rmse = mean_squared_error(y_test, y_pred_optimized, squared=False)
r2 = r2_score(y_test, y_pred_optimized)

print("R-squared:", r2)
print("Root Mean Squared Error:", rmse)
print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)

"""## **Feature importance**"""

# Obtain feature importances
feature_importances = best_dt_model.feature_importances_

# Create a DataFrame to store feature importances along with their names
feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance values in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Visualize feature importances
plt.figure(figsize=(10, 10))
sns.barplot(data=feature_importance_df, x='Importance', y='Feature')
plt.title('Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

"""## **Cross validation**"""

# Define custom scoring functions for R-squared, RMSE, MSE, and MAE
def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

def mse(y_true, y_pred):
    return mean_squared_error(y_true, y_pred)

def mae(y_true, y_pred):
    return mean_absolute_error(y_true, y_pred)

# Make scorer objects for each custom scoring function
r2_scorer = make_scorer(r2_score)
rmse_scorer = make_scorer(rmse, greater_is_better=False)  # RMSE is better when lower
mse_scorer = make_scorer(mse, greater_is_better=False)    # MSE is better when lower
mae_scorer = make_scorer(mae, greater_is_better=False)    # MAE is better when lower

# Perform cross-validation with custom scoring functions
cv_r2 = cross_val_score(dt_model, X_train, y_train, cv=5, scoring=r2_scorer)
cv_rmse = -cross_val_score(dt_model, X_train, y_train, cv=5, scoring=rmse_scorer)
cv_mse = -cross_val_score(dt_model, X_train, y_train, cv=5, scoring=mse_scorer)
cv_mae = -cross_val_score(dt_model, X_train, y_train, cv=5, scoring=mae_scorer)

# Print the cross-validation scores
print("Cross-Validation R-squared:", cv_r2)
print("Mean R-squared:", np.mean(cv_r2))
print("Cross-Validation RMSE:", cv_rmse)
print("Mean RMSE:", np.mean(cv_rmse))
print("Cross-Validation MSE:", cv_mse)
print("Mean MSE:", np.mean(cv_mse))
print("Cross-Validation MAE:", cv_mae)
print("Mean MAE:", np.mean(cv_mae))

"""# **Visualization**"""

# Plot actual prices versus default prediction versus optimized prediction
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_default, color='blue', label='Default Prediction')
plt.scatter(y_test, y_pred_optimized, color='green', label='Optimized Prediction')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, color='red')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual Prices vs Default Prediction vs Optimized Prediction (Decision Tree Regressor)')
plt.legend()
plt.show()

# Randomly select 250 samples from the test set
np.random.seed(42)
sample_indices = np.random.choice(len(y_test), size=250, replace=False)
sample_y_test = y_test.iloc[sample_indices]
sample_y_pred_default = y_pred_default[sample_indices]
sample_y_pred_optimized = y_pred_optimized[sample_indices]

# Plot the sample of predicted prices versus actual prices
plt.figure(figsize=(18, 8))
plt.plot(range(len(sample_y_test)), sample_y_test, label='Actual Prices', color='blue')
plt.plot(range(len(sample_y_test)), sample_y_pred_default, label='DT Default Predictions', color='green')
plt.plot(range(len(sample_y_test)), sample_y_pred_optimized, label='DT Optimized Predictions', color='red')
plt.xlabel('Sample Index')
plt.ylabel('Price Paid (Â£)')
plt.title('250 Sample Predicted Prices vs Actual Prices (Decision Tree Regressor)')
plt.legend()
plt.show()

"""# **Gradient Boosting Regressor**

# **Default**
"""

# Train the model
gb_model = GradientBoostingRegressor(random_state=42)
gb_model.fit(X_train, y_train)

# Make predictions
y_pred = gb_model.predict(X_test)

# Calculate evaluation metrics
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)
mae = mean_absolute_error(y_test, y_pred)

print("Evaluation Metrics:")
print("R-squared:", r2)
print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("Mean Absolute Error:", mae)

"""## **Feature importance**"""

# Obtain feature importances
feature_importances = gb_model.feature_importances_

# Create a DataFrame to store feature importances along with their names
feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance values in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Visualize feature importances
plt.figure(figsize=(10, 10))
sns.barplot(data=feature_importance_df, x='Importance', y='Feature')
plt.title('Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

"""## **Cross validation**"""

# Define custom scoring functions for R-squared, RMSE, MSE, and MAE
def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

def mse(y_true, y_pred):
    return mean_squared_error(y_true, y_pred)

def mae(y_true, y_pred):
    return mean_absolute_error(y_true, y_pred)

# Make scorer objects for each custom scoring function
r2_scorer = make_scorer(r2_score)
rmse_scorer = make_scorer(rmse, greater_is_better=False)  # RMSE is better when lower
mse_scorer = make_scorer(mse, greater_is_better=False)    # MSE is better when lower
mae_scorer = make_scorer(mae, greater_is_better=False)    # MAE is better when lower

# Perform cross-validation with custom scoring functions
cv_r2 = cross_val_score(gb_model, X_train, y_train, cv=5, scoring=r2_scorer)
cv_rmse = -cross_val_score(gb_model, X_train, y_train, cv=5, scoring=rmse_scorer)
cv_mse = -cross_val_score(gb_model, X_train, y_train, cv=5, scoring=mse_scorer)
cv_mae = -cross_val_score(gb_model, X_train, y_train, cv=5, scoring=mae_scorer)

# Print the cross-validation scores
print("Cross-Validation R-squared:", cv_r2)
print("Mean R-squared:", np.mean(cv_r2))
print("Cross-Validation RMSE:", cv_rmse)
print("Mean RMSE:", np.mean(cv_rmse))
print("Cross-Validation MSE:", cv_mse)
print("Mean MSE:", np.mean(cv_mse))
print("Cross-Validation MAE:", cv_mae)
print("Mean MAE:", np.mean(cv_mae))

import pandas as pd

# Define the scores for each model
scores = [
    {"Default_Model": "Linear Regression", "R-squared": 0.26, "RMSE": 271325, "MSE": 73617192326, "MAE": 191584},
    {"Default_Model": "XGBoost", "R-squared": 0.53, "RMSE": 216040, "MSE": 46673438837, "MAE": 143841},
    {"Default_Model": "Random Forest", "R-squared": 0.66, "RMSE": 184666, "MSE": 34101668716, "MAE": 120770},
    {"Default_Model": "Decision Tree", "R-squared": 0.27, "RMSE": 268447, "MSE": 72063746775, "MAE": 172318},
    {"Default_Model": "Gradient Boosting", "R-squared": 0.46, "RMSE": 232387, "MSE": 54003658847, "MAE": 153994}
]


# Convert scores to DataFrame
df_scores = pd.DataFrame(scores)

# Define colors for each model
color_mapping = {
    "Linear Regression": "lightblue",
    "XGBoost": "lightgreen",
    "Random Forest": "lightpink",
    "Decision Tree": "lightyellow",
    "Gradient Boosting": "lightcyan"
}

# Sort the DataFrame by R-squared in descending order
df_sorted = df_scores.sort_values(by='RMSE', ascending=True)

# Define function to apply background color based on model
def colorize(row):
    model = row["Default_Model"]
    color = color_mapping.get(model, "white")  # Default to white if model not found in mapping
    return [f'background-color: {color}'] * len(row)

# Apply colorize function to each row
df_styled = df_scores.style.apply(colorize, axis=1)

# Display styled DataFrame
df_styled

import pandas as pd
import numpy as np

# Define the scores for each model
scores = [
    {"Optimized_Model": "XGBoost", "R-squared": 0.61, "RMSE": 197368, "MSE": 38953942063, "MAE": 131680},
    {"Optimized_Model": "Random Forest Regressor", "R-squared": 0.62, "RMSE": 193327, "MSE": 37375511303, "MAE": 127629},
    {"Optimized_Model": "Decision Tree Regressor", "R-squared": 0.53, "RMSE": 216538, "MSE": 46888898114, "MAE": 142628}
]


# Convert scores to DataFrame
df_scores = pd.DataFrame(scores)

# Define colors for each model
color_mapping = {
    "XGBoost": "lightblue",
    "Random Forest Regressor": "lightgreen",
    "Decision Tree Regressor": "lightpink"
}

# Sort the DataFrame by R-squared in descending order
df_sorted = df_scores.sort_values(by='RMSE', ascending=True)

# Define function to apply background color based on model
def colorize(row):
    model = row["Optimized_Model"]
    color = color_mapping.get(model, "white")  # Default to white if model not found in mapping
    return [f'background-color: {color}'] * len(row)

# Apply colorize function to each row
df_styled = df_scores.style.apply(colorize, axis=1)


# Display styled DataFrame
df_styled